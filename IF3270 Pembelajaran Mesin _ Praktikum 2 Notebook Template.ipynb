{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"markdown","source":["# IF3270 Machine Learning | Praktikum\n","\n","This notebook serves as a template for the assignment. Please create a copy of this notebook to complete your work. You can add more code blocks, markdown blocks, or new sections if needed.\n"],"metadata":{"id":"uR1JW69eLfG_"}},{"cell_type":"markdown","source":["Group Number: xx\n","\n","Group Members:\n","- Name (NIM)\n","- Name (NIM)"],"metadata":{"id":"ucbaI5rBLtjJ"}},{"cell_type":"markdown","source":["## Import Libraries"],"metadata":{"id":"GwzsfETHLfHA"}},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","\n","# Import other libraries if needed"],"metadata":{"id":"jZJU5W_4LfHB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Import Dataset"],"metadata":{"id":"OKbjLIdYLfHC"}},{"cell_type":"code","source":["# Write your code here"],"metadata":{"id":"-IWFJ-gdLfHD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Exploratory Data Analysis\n","\n","Exploratory Data Analysis (EDA) is a crucial step in the data analysis process that involves examining and visualizing data sets to uncover patterns, trends, anomalies, and insights. It is the first step before applying more advanced statistical and machine learning techniques. EDA helps you to gain a deep understanding of the data you are working with, allowing you to make informed decisions and formulate hypotheses for further analysis. Provide at least 2 analysis."],"metadata":{"id":"YdSor5sdIYGs"}},{"cell_type":"code","source":["# Write your code here"],"metadata":{"id":"bGiGPVYNIoWk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 2. Split Training Set and Validation Set\n","\n","Splitting the training and validation set works as an early diagnostic towards the performance of the model we train. This is done before the preprocessing steps to **avoid data leakage inbetween the sets**. If you want to use k-fold cross-validation, split the data later and do the cleaning and preprocessing separately for each split."],"metadata":{"id":"gvx-gT3bLfHM"}},{"cell_type":"code","source":["# Split training set and validation set here, store into variables train_set and val_set.\n","# Remember to also keep the original training set before splitting. This will come important later.\n","# train_set, val_set = ..."],"metadata":{"id":"4yWCUFFBLfHM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 3. Data Cleaning and Preprocessing\n","\n","This step is the first thing to be done once a Data Scientist have grasped a general knowledge of the data. Raw data is **seldom ready for training**, therefore steps need to be taken to clean and format the data for the Machine Learning model to interpret.\n","\n","By performing data cleaning and preprocessing, you ensure that your dataset is ready for model training, leading to more accurate and reliable machine learning results. These steps are essential for transforming raw data into a format that machine learning algorithms can effectively learn from and make predictions.\n","\n","For each step that you will do, **please explain the reason why did you do that process. Write it in a markdown cell under the code cell you wrote.**"],"metadata":{"id":"IC14lmo_LfHN"}},{"cell_type":"code","source":["# Write your code here"],"metadata":{"id":"5rksSMAWICY_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Compile Preprocessing Pipeline\n","\n","All of the preprocessing classes or functions defined earlier will be compiled in this step."],"metadata":{"id":"-ctVzt5DLfHd"}},{"cell_type":"markdown","source":["If you use sklearn to create preprocessing classes, you can list your preprocessing classes in the Pipeline object sequentially, and then fit and transform your data."],"metadata":{"id":"S_ZlncSVjJG6"}},{"cell_type":"code","source":["# from sklearn.pipeline import Pipeline\n","\n","# # Note: You can add or delete preprocessing components from this pipeline\n","\n","# pipe = Pipeline([(\"imputer\", FeatureImputer()),\n","#                  (\"featurecreator\", FeatureCreator()),\n","#                  (\"scaler\", FeatureScaler()),\n","#                  (\"encoder\", FeatureEncoder())])\n","\n","# train_set = pipe.fit_transform(train_set)\n","# val_set = pipe.transform(val_set)"],"metadata":{"id":"jHraoW_7LfHd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # Your code should work up until this point\n","# train_set = pipe.fit_transform(train_set)\n","# val_set = pipe.transform(val_set)"],"metadata":{"id":"9s56aFFxLfHd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["or create your own here"],"metadata":{"id":"SXoCqMztjhr-"}},{"cell_type":"code","source":["# Write your code here"],"metadata":{"id":"7OoZ3oXEj2CW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Modeling and Validation\n","\n","Modeling is the process of building a machine learning model to solve a particular problem, or in the context of this task, predicting each class in an image using a `Convolutional Neural Network with AlexNet Architecture`. Validation is the process of evaluating your trained model using the validation set or cross-validation method and providing some metrics that can help you decide what to do in the next iteration of development."],"metadata":{"id":"9A3adbZXLfHe"}},{"cell_type":"markdown","source":["## 4.1 Convolutional Neural Network\n","\n","You need to build and compare two models:\n","- Build your own model with AlexNet architecture using PyTorch/Tensorflow.\n","- Fine-tune a pretrained model (huggingface/roboflow)\n"],"metadata":{"id":"cRWB1ua4pvYu"}},{"cell_type":"markdown","source":["### 4.1.1 Build Your Own Model\n","\n","Convolutional Neural Network (CNN) is a type of artificial neural network specifically designed to process data that has a grid-like structure, such as images or digital pictures. AlexNet is one of the well-known CNN architectures due to its success in winning the ImageNet competition in 2012. AlexNet consists of several convolutional layers accompanied by pooling layers and fully connected layers, and it uses the ReLU activation function. Try to create your own CNN model with AlexNet architecture using PyTorch/Tensorflow."],"metadata":{"id":"h49yUXXbqmfV"}},{"cell_type":"code","source":["# Type your code here"],"metadata":{"id":"Lh9n6lcqTyRI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 4.1.2 Pretrained Model Fine-tuning\n","\n","Aside from building your own CNN model, you can also fine-tune a pretrained model from platforms like Hugging Face or Roboflow. As a reference, you may use `ResNet50`, one of the most widely used pretrained CNN architectures."],"metadata":{"id":"2jK7JcDorGpt"}},{"cell_type":"code","source":["# Type your code here"],"metadata":{"id":"k_-RGnwJTxvR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.2 Validation\n","\n","Validation is the process of evaluating a trained model using a validation set or cross-validation method. It provides metrics that help determine the necessary steps for the next iteration of model development.  \n","\n","For validation, the metric used is **macro f1-score**. A higher f1-score indicates better model calibration.  \n","\n","### Required Validation Results  \n","The validation results that must be included in the notebook are:  \n","1. The validation results from the required baseline models.  \n","2. The validation results from the final submission model on **Kaggle**.  "],"metadata":{"id":"1lPeNCJohenc"}},{"cell_type":"code","source":["# Type your code here"],"metadata":{"id":"68IiNejlTuke"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Submission\n","To predict the test set target feature and submit the results to the kaggle competition platform, do the following:\n","1. Create a new pipeline instance identical to the first in Data Preprocessing\n","2. With the pipeline, apply `fit_transform` to the original training set before splitting, then only apply `transform` to the test set.\n","3. Retrain the model on the preprocessed training set\n","4. Predict the test set"],"metadata":{"id":"Li4l53DjLfHh"}},{"cell_type":"code","source":["# Type your code here"],"metadata":{"id":"LeqnfWc-LfHi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Error Analysis\n","\n","Based on all the process you have done until the modeling and evaluation step, write an analysis to support each steps you have taken to solve this problem. Write the analysis using the markdown block. Some questions that may help you in writing the analysis:\n","\n","1. What is the error distribution between classes? Do most misclassifications come from one class?\n","2. Are there more false positives or false negatives?\n","3. Do occlusions or unusual angles contribute to errors?\n","4. Are the misclassifications random or do they follow a certain pattern?\n","5. etc.."],"metadata":{"id":"R-jXvKOpLfHi"}},{"cell_type":"markdown","source":["`Provide your analysis here`"],"metadata":{"id":"tWL3nEAELfHj"}},{"cell_type":"markdown","source":["# 6. Insights\n","\n","For each step you have done, do an analysis and mention the insights obtained."],"metadata":{"id":"wvS-lGcM_1sK"}},{"cell_type":"markdown","source":["`Provide your insights here`"],"metadata":{"id":"UR2Vy0DsAHan"}}]}